# å›¾åƒè¶…åˆ†è¾¨ç‡è¯¦è§£ ğŸ”

> å›¾åƒè¶…åˆ†è¾¨ç‡å°±åƒæ˜¯æ•°å­—ä¸–ç•Œçš„"æ™ºèƒ½æ”¾å¤§é•œ"ï¼é€šè¿‡å„ç§"æ”¾å¤§æŠ€æœ¯"ï¼Œæˆ‘ä»¬å¯ä»¥è®©ä½åˆ†è¾¨ç‡çš„å›¾åƒå˜å¾—æ›´åŠ æ¸…æ™°ï¼Œå°±åƒä½¿ç”¨æ”¾å¤§é•œè§‚å¯Ÿç»†èŠ‚ä¸€æ ·ã€‚è®©æˆ‘ä»¬ä¸€èµ·æ¥æ¢ç´¢è¿™ä¸ªç¥å¥‡çš„å›¾åƒ"æ”¾å¤§å·¥ä½œå®¤"å§ï¼

## ç›®å½•

- [1. ä»€ä¹ˆæ˜¯å›¾åƒè¶…åˆ†è¾¨ç‡ï¼Ÿ](#1-ä»€ä¹ˆæ˜¯å›¾åƒè¶…åˆ†è¾¨ç‡)
- [2. åŒä¸‰æ¬¡æ’å€¼è¶…åˆ†è¾¨ç‡](#2-åŒä¸‰æ¬¡æ’å€¼è¶…åˆ†è¾¨ç‡)
- [3. åŸºäºç¨€ç–è¡¨ç¤ºçš„è¶…åˆ†è¾¨ç‡](#3-åŸºäºç¨€ç–è¡¨ç¤ºçš„è¶…åˆ†è¾¨ç‡)
- [4. åŸºäºæ·±åº¦å­¦ä¹ çš„è¶…åˆ†è¾¨ç‡](#4-åŸºäºæ·±åº¦å­¦ä¹ çš„è¶…åˆ†è¾¨ç‡)
- [5. å¤šå¸§è¶…åˆ†è¾¨ç‡](#5-å¤šå¸§è¶…åˆ†è¾¨ç‡)
- [6. å®æ—¶è¶…åˆ†è¾¨ç‡](#6-å®æ—¶è¶…åˆ†è¾¨ç‡)
- [æ€»ç»“](#æ€»ç»“)
- [å‚è€ƒèµ„æ–™](#å‚è€ƒèµ„æ–™)

## 1. ä»€ä¹ˆæ˜¯å›¾åƒè¶…åˆ†è¾¨ç‡ï¼Ÿ

å›¾åƒè¶…åˆ†è¾¨ç‡å°±åƒæ˜¯æ•°å­—ä¸–ç•Œçš„"æ™ºèƒ½æ”¾å¤§é•œ"ï¼Œä¸»è¦ç›®çš„æ˜¯ï¼š
- ğŸ” æå‡å›¾åƒåˆ†è¾¨ç‡ï¼ˆå°±åƒæ”¾å¤§é•œæ”¾å¤§ç»†èŠ‚ï¼‰
- ğŸ–¼ï¸ æ¢å¤å›¾åƒç»†èŠ‚ï¼ˆå°±åƒé‡ç°ä¸¢å¤±çš„çº¹ç†ï¼‰
- ğŸ“ˆ æ”¹å–„å›¾åƒè´¨é‡ï¼ˆå°±åƒæå‡è§‚å¯Ÿæ¸…æ™°åº¦ï¼‰
- ğŸ¯ æ‰©å±•åº”ç”¨åœºæ™¯ï¼ˆå°±åƒæ‰©å¤§ä½¿ç”¨èŒƒå›´ï¼‰

å¸¸è§çš„è¶…åˆ†è¾¨ç‡æ–¹æ³•åŒ…æ‹¬ï¼š
- ä¼ ç»Ÿæ’å€¼æ–¹æ³•ï¼ˆæœ€åŸºç¡€çš„"æ”¾å¤§å·¥å…·"ï¼‰
- åŸºäºé‡å»ºçš„æ–¹æ³•ï¼ˆæ™ºèƒ½"ç»†èŠ‚é‡å»º"ï¼‰
- åŸºäºå­¦ä¹ çš„æ–¹æ³•ï¼ˆæ•°æ®é©±åŠ¨"æ”¾å¤§"ï¼‰
- æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼ˆAI"æ™ºèƒ½æ”¾å¤§"ï¼‰

## 2. åŒä¸‰æ¬¡æ’å€¼è¶…åˆ†è¾¨ç‡

### 2.1 ç®—æ³•åŸç†

åŒä¸‰æ¬¡æ’å€¼å°±åƒæ˜¯ä½¿ç”¨"æ™ºèƒ½æ”¾å¤§é•œ"ï¼Œé€šè¿‡è®¡ç®—16ä¸ªç›¸é‚»åƒç´ çš„åŠ æƒå¹³å‡æ¥é‡å»ºé«˜åˆ†è¾¨ç‡å›¾åƒã€‚å®ƒæ¯”åŒçº¿æ€§æ’å€¼æ›´ç²¾ç¡®ï¼Œèƒ½å¤Ÿäº§ç”Ÿæ›´å¹³æ»‘çš„ç»“æœã€‚

æ•°å­¦è¡¨è¾¾å¼ï¼š
$$
I_{HR}(x,y) = \sum_{i,j} I_{LR}(i,j) \cdot K(x-i, y-j)
$$

å…¶ä¸­ï¼š
- $I_{HR}$ æ˜¯é«˜åˆ†è¾¨ç‡å›¾åƒ
- $I_{LR}$ æ˜¯ä½åˆ†è¾¨ç‡å›¾åƒ
- $K$ æ˜¯åŒä¸‰æ¬¡æ’å€¼æ ¸å‡½æ•°

### 2.2 ä»£ç å®ç°

#### C++å®ç°
```cpp
// åŒä¸‰æ¬¡æ’å€¼æ ¸å‡½æ•°
double bicubic_kernel(double x) {
    x = abs(x);
    if(x <= 1.0) {
        return 1.5*x*x*x - 2.5*x*x + 1.0;
    }
    else if(x < 2.0) {
        return -0.5*x*x*x + 2.5*x*x - 4.0*x + 2.0;
    }
    return 0.0;
}

Mat bicubic_sr(const Mat& src, float scale_factor) {
    int new_rows = static_cast<int>(round(src.rows * scale_factor));
    int new_cols = static_cast<int>(round(src.cols * scale_factor));
    Mat dst(new_rows, new_cols, src.type());

    // Process each channel separately
    vector<Mat> channels;
    split(src, channels);
    vector<Mat> upscaled_channels;

    #pragma omp parallel for
    for(int c = 0; c < static_cast<int>(channels.size()); c++) {
        Mat upscaled(new_rows, new_cols, CV_32F);

        // Bicubic interpolation
        for(int i = 0; i < new_rows; i++) {
            float y = i / scale_factor;
            int y0 = static_cast<int>(floor(y));

            for(int j = 0; j < new_cols; j++) {
                float x = j / scale_factor;
                int x0 = static_cast<int>(floor(x));

                double sum = 0;
                double weight_sum = 0;

                // 4x4 neighborhood interpolation
                for(int di = -1; di <= 2; di++) {
                    int yi = y0 + di;
                    if(yi < 0 || yi >= src.rows) continue;

                    double wy = bicubic_kernel(y - yi);

                    for(int dj = -1; dj <= 2; dj++) {
                        int xj = x0 + dj;
                        if(xj < 0 || xj >= src.cols) continue;

                        double wx = bicubic_kernel(x - xj);
                        double w = wx * wy;

                        sum += w * channels[c].at<uchar>(yi,xj);
                        weight_sum += w;
                    }
                }

                upscaled.at<float>(i,j) = static_cast<float>(sum / weight_sum);
            }
        }

        upscaled.convertTo(upscaled, CV_8U);
        upscaled_channels.push_back(upscaled);
    }

    merge(upscaled_channels, dst);
    return dst;
}
```

#### Pythonå®ç°
```python
def bicubic_interpolation(src: np.ndarray, scale: float = 2.0) -> np.ndarray:
    """åŒä¸‰æ¬¡æ’å€¼è¶…åˆ†è¾¨ç‡

    Args:
        src: è¾“å…¥å›¾åƒ
        scale: æ”¾å¤§å€æ•°

    Returns:
        np.ndarray: è¶…åˆ†è¾¨ç‡åçš„å›¾åƒ
    """
    # è®¡ç®—è¾“å‡ºå›¾åƒå¤§å°
    h, w = src.shape[:2]
    new_h, new_w = int(h * scale), int(w * scale)

    # åˆ›å»ºè¾“å‡ºå›¾åƒ
    dst = np.zeros((new_h, new_w, 3), dtype=np.uint8)

    # åŒä¸‰æ¬¡æ’å€¼æ ¸å‡½æ•°
    def bicubic_kernel(x: float) -> float:
        x = abs(x)
        if x < 1:
            return 1 - 2 * x**2 + x**3
        elif x < 2:
            return 4 - 8 * x + 5 * x**2 - x**3
        else:
            return 0

    # å¯¹æ¯ä¸ªè¾“å‡ºåƒç´ è¿›è¡Œæ’å€¼
    for i in range(new_h):
        for j in range(new_w):
            # è®¡ç®—å¯¹åº”çš„è¾“å…¥å›¾åƒåæ ‡
            x = j / scale
            y = i / scale

            # è·å–16ä¸ªç›¸é‚»åƒç´ 
            x0 = int(x)
            y0 = int(y)
            x1 = min(x0 + 1, w - 1)
            y1 = min(y0 + 1, h - 1)

            # è®¡ç®—æƒé‡
            wx = [bicubic_kernel(x - (x0-1)), bicubic_kernel(x - x0),
                  bicubic_kernel(x - x1), bicubic_kernel(x - (x1+1))]
            wy = [bicubic_kernel(y - (y0-1)), bicubic_kernel(y - y0),
                  bicubic_kernel(y - y1), bicubic_kernel(y - (y1+1))]

            # è®¡ç®—æ’å€¼ç»“æœ
            for c in range(3):
                val = 0
                for dy in range(-1, 3):
                    for dx in range(-1, 3):
                        if (0 <= y0+dy < h and 0 <= x0+dx < w):
                            val += src[y0+dy, x0+dx, c] * wx[dx+1] * wy[dy+1]
                dst[i, j, c] = np.clip(val, 0, 255)

    return dst
```

## 3. åŸºäºç¨€ç–è¡¨ç¤ºçš„è¶…åˆ†è¾¨ç‡

### 3.1 ç®—æ³•åŸç†

åŸºäºç¨€ç–è¡¨ç¤ºçš„è¶…åˆ†è¾¨ç‡å°±åƒæ˜¯ä½¿ç”¨"æ™ºèƒ½æ‹¼å›¾"ï¼Œé€šè¿‡å­—å…¸å­¦ä¹ å°†å›¾åƒå—è¡¨ç¤ºä¸ºç¨€ç–ç³»æ•°çš„ç»„åˆã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´å¥½åœ°ä¿æŒå›¾åƒç»†èŠ‚å’Œçº¹ç†ã€‚

ä¼˜åŒ–ç›®æ ‡ï¼š
$$
\min_x \|y - Ax\|^2 + \lambda R(x)
$$

å…¶ä¸­ï¼š
- $x$ æ˜¯å¾…é‡å»ºçš„é«˜åˆ†è¾¨ç‡å›¾åƒ
- $y$ æ˜¯è§‚å¯Ÿåˆ°çš„ä½åˆ†è¾¨ç‡å›¾åƒ
- $A$ æ˜¯é™è´¨è¿‡ç¨‹
- $R(x)$ æ˜¯æ­£åˆ™åŒ–é¡¹

### 3.2 ä»£ç å®ç°

#### C++å®ç°
```cpp
Mat sparse_sr(
    const Mat& src,
    float scale_factor,
    int dict_size,
    int patch_size) {

    // Use bicubic interpolation as initial estimate
    Mat initial = bicubic_sr(src, scale_factor);
    Mat result = initial.clone();

    // Extract training samples
    vector<Mat> patches;
    for(int i = 0; i <= src.rows-patch_size; i++) {
        for(int j = 0; j <= src.cols-patch_size; j++) {
            Mat patch = src(Rect(j,i,patch_size,patch_size));
            patches.push_back(patch.clone());
        }
    }

    // Train dictionary
    Mat dictionary(dict_size, patch_size*patch_size, CV_32F);
    for(int i = 0; i < dict_size; i++) {
        int idx = rand() % static_cast<int>(patches.size());
        Mat feat = extract_patch_features(patches[idx]);
        feat.copyTo(dictionary.row(i));
    }

    // Sparse reconstruction for each patch
    #pragma omp parallel for
    for(int i = 0; i < result.rows-patch_size; i++) {
        for(int j = 0; j < result.cols-patch_size; j++) {
            Mat patch = result(Rect(j,i,patch_size,patch_size));
            Mat features = extract_patch_features(patch);

            // Find the most similar dictionary atom
            double min_dist = numeric_limits<double>::max();
            Mat best_atom;

            for(int k = 0; k < dict_size; k++) {
                Mat atom = dictionary.row(k);
                double dist = norm(features, atom);
                if(dist < min_dist) {
                    min_dist = dist;
                    best_atom = atom;
                }
            }

            // Reconstruction
            Mat reconstructed;
            idct(best_atom.reshape(1,patch_size), reconstructed);
            reconstructed.copyTo(result(Rect(j,i,patch_size,patch_size)));
        }
    }

    return result;
}
```

#### Pythonå®ç°
```python
def sparse_super_resolution(src: np.ndarray, scale: float = 2.0,
                          lambda_: float = 0.1) -> np.ndarray:
    """åŸºäºç¨€ç–è¡¨ç¤ºçš„è¶…åˆ†è¾¨ç‡

    Args:
        src: è¾“å…¥å›¾åƒ
        scale: æ”¾å¤§å€æ•°
        lambda_: æ­£åˆ™åŒ–å‚æ•°

    Returns:
        np.ndarray: è¶…åˆ†è¾¨ç‡åçš„å›¾åƒ
    """
    # è®¡ç®—è¾“å‡ºå›¾åƒå¤§å°
    h, w = src.shape[:2]
    new_h, new_w = int(h * scale), int(w * scale)

    # åˆ›å»ºè¾“å‡ºå›¾åƒ
    dst = np.zeros((new_h, new_w, 3), dtype=np.uint8)

    # æ„å»ºç¨€ç–è¡¨ç¤ºçŸ©é˜µ
    def build_sparse_matrix(h: int, w: int) -> sparse.csr_matrix:
        n = h * w
        data = []
        row = []
        col = []

        # æ·»åŠ æ¢¯åº¦çº¦æŸ
        for i in range(h):
            for j in range(w):
                idx = i * w + j
                if i > 0:
                    data.extend([1, -1])
                    row.extend([idx, idx])
                    col.extend([idx, (i-1)*w+j])
                if j > 0:
                    data.extend([1, -1])
                    row.extend([idx, idx])
                    col.extend([idx, i*w+j-1])

        return sparse.csr_matrix((data, (row, col)), shape=(n, n))

    # å¯¹æ¯ä¸ªé€šé“è¿›è¡Œå¤„ç†
    for c in range(3):
        # æ„å»ºç¨€ç–çŸ©é˜µ
        A = build_sparse_matrix(new_h, new_w)

        # æ„å»ºç›®æ ‡å‘é‡
        b = src[:,:,c].flatten()

        # æ±‚è§£ç¨€ç–è¡¨ç¤º
        x = spsolve(A + lambda_ * sparse.eye(new_h*new_w), b)

        # é‡æ„å›¾åƒ
        dst[:,:,c] = x.reshape(new_h, new_w)

    return dst.astype(np.uint8)
```

## 4. åŸºäºæ·±åº¦å­¦ä¹ çš„è¶…åˆ†è¾¨ç‡

### 4.1 ç®—æ³•åŸç†

æ·±åº¦å­¦ä¹ è¶…åˆ†è¾¨ç‡å°±åƒæ˜¯è®­ç»ƒä¸€ä¸ª"AIæ”¾å¤§é•œ"ï¼Œé€šè¿‡ç¥ç»ç½‘ç»œå­¦ä¹ ä»ä½åˆ†è¾¨ç‡åˆ°é«˜åˆ†è¾¨ç‡çš„æ˜ å°„å…³ç³»ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿå­¦ä¹ åˆ°æ›´å¤æ‚çš„å›¾åƒç‰¹å¾å’Œç»†èŠ‚ã€‚

### 4.2 ä»£ç å®ç°

#### C++å®ç°
```cpp
Mat srcnn_sr(const Mat& src, float scale_factor) {
    // Use bicubic interpolation as initial estimate
    Mat initial = bicubic_sr(src, scale_factor);
    Mat result = initial.clone();

    // SRCNN network parameters (simplified version)
    const int conv1_size = 9;
    const int conv2_size = 1;
    const int conv3_size = 5;

    // First convolution layer
    Mat conv1;
    Mat kernel1 = getGaussianKernel(conv1_size, -1);
    kernel1 = kernel1 * kernel1.t();
    filter2D(result, conv1, -1, kernel1);

    // Second convolution layer (1x1 convolution for non-linear mapping)
    Mat conv2;
    Mat kernel2 = Mat::ones(conv2_size, conv2_size, CV_32F) / static_cast<float>(conv2_size*conv2_size);
    filter2D(conv1, conv2, -1, kernel2);

    // Third convolution layer (reconstruction)
    Mat conv3;
    Mat kernel3 = getGaussianKernel(conv3_size, -1);
    kernel3 = kernel3 * kernel3.t();
    filter2D(conv2, conv3, -1, kernel3);

    // Residual learning
    result = conv3 + initial;

    return result;
}
```

#### Pythonå®ç°
```python
def deep_learning_super_resolution(src: np.ndarray, scale: float = 2.0,
                                 model_path: Optional[str] = None) -> np.ndarray:
    """åŸºäºæ·±åº¦å­¦ä¹ çš„è¶…åˆ†è¾¨ç‡

    Args:
        src: è¾“å…¥å›¾åƒ
        scale: æ”¾å¤§å€æ•°
        model_path: é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„

    Returns:
        np.ndarray: è¶…åˆ†è¾¨ç‡åçš„å›¾åƒ
    """
    # è¿™é‡Œä½¿ç”¨ç®€åŒ–çš„SRCNNç»“æ„
    class SRCNN:
        def __init__(self):
            self.conv1 = cv2.dnn.readNetFromCaffe(
                'srcnn.prototxt', 'srcnn.caffemodel')

        def forward(self, img: np.ndarray) -> np.ndarray:
            # é¢„å¤„ç†
            blob = cv2.dnn.blobFromImage(img, 1.0/255.0)

            # å‰å‘ä¼ æ’­
            self.conv1.setInput(blob)
            output = self.conv1.forward()

            # åå¤„ç†
            output = output[0].transpose(1, 2, 0)
            output = np.clip(output * 255, 0, 255).astype(np.uint8)

            return output

    # åˆ›å»ºæ¨¡å‹
    model = SRCNN()

    # è¶…åˆ†è¾¨ç‡å¤„ç†
    dst = model.forward(src)

    return dst
```

## 5. å¤šå¸§è¶…åˆ†è¾¨ç‡

### 5.1 ç®—æ³•åŸç†

å¤šå¸§è¶…åˆ†è¾¨ç‡å°±åƒæ˜¯"åŠ¨æ€æ”¾å¤§é•œ"ï¼Œé€šè¿‡èåˆå¤šå¸§å›¾åƒçš„ä¿¡æ¯æ¥é‡å»ºé«˜åˆ†è¾¨ç‡å›¾åƒã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿåˆ©ç”¨æ—¶é—´ç»´åº¦çš„ä¿¡æ¯ï¼Œè·å¾—æ›´å¥½çš„é‡å»ºæ•ˆæœã€‚

### 5.2 ä»£ç å®ç°

#### C++å®ç°
```cpp
Mat multi_frame_sr(
    const vector<Mat>& frames,
    float scale_factor) {

    if(frames.empty()) return Mat();

    // Select reference frame
    Mat reference = frames[frames.size()/2];
    Size new_size(static_cast<int>(round(reference.cols * scale_factor)),
                  static_cast<int>(round(reference.rows * scale_factor)));

    // Initial estimate
    Mat result = bicubic_sr(reference, scale_factor);

    // Registration and fusion for each frame
    for(const Mat& frame : frames) {
        if(frame.empty()) continue;
        if(frame.size() != reference.size()) continue;

        // Calculate optical flow
        Mat flow;
        calcOpticalFlowFarneback(reference, frame, flow, 0.5, 3, 15, 3, 5, 1.2, 0);

        // Register based on flow
        Mat warped;
        remap(frame, warped, flow, Mat(), INTER_LINEAR);

        // Upscale registered frame
        Mat upscaled = bicubic_sr(warped, scale_factor);

        // Weighted fusion
        double alpha = 0.5;
        addWeighted(result, 1-alpha, upscaled, alpha, 0, result);
    }

    return result;
}
```

#### Pythonå®ç°
```python
def multi_frame_super_resolution(frames: List[np.ndarray],
                               scale: float = 2.0) -> np.ndarray:
    """å¤šå¸§è¶…åˆ†è¾¨ç‡

    Args:
        frames: è¾“å…¥è§†é¢‘å¸§åˆ—è¡¨
        scale: æ”¾å¤§å€æ•°

    Returns:
        np.ndarray: è¶…åˆ†è¾¨ç‡åçš„å›¾åƒ
    """
    # è®¡ç®—è¾“å‡ºå›¾åƒå¤§å°
    h, w = frames[0].shape[:2]
    new_h, new_w = int(h * scale), int(w * scale)

    # åˆ›å»ºè¾“å‡ºå›¾åƒ
    dst = np.zeros((new_h, new_w, 3), dtype=np.float32)

    # è®¡ç®—å…‰æµåœº
    flows = []
    for i in range(len(frames)-1):
        flow = cv2.calcOpticalFlowFarneback(
            frames[i], frames[i+1], None, 0.5, 3, 15, 3, 5, 1.2, 0)
        flows.append(flow)

    # å¯¹æ¯ä¸€å¸§è¿›è¡Œé…å‡†å’Œèåˆ
    for i, frame in enumerate(frames):
        # åŒä¸‰æ¬¡æ’å€¼
        upscaled = bicubic_interpolation(frame, scale)

        # è®¡ç®—é…å‡†åç§»
        if i > 0:
            flow = flows[i-1] * scale
            upscaled = cv2.remap(upscaled, flow[:,:,0], flow[:,:,1],
                                cv2.INTER_LINEAR)

        # ç´¯åŠ 
        dst += upscaled.astype(np.float32)

    # å¹³å‡
    dst /= len(frames)

    return dst.astype(np.uint8)
```

## 6. å®æ—¶è¶…åˆ†è¾¨ç‡

### 6.1 ç®—æ³•åŸç†

å®æ—¶è¶…åˆ†è¾¨ç‡å°±åƒæ˜¯"å¿«é€Ÿæ”¾å¤§é•œ"ï¼Œé€šè¿‡ä¼˜åŒ–ç®—æ³•å®ç°å®æ—¶å¤„ç†ã€‚è¿™ç§æ–¹æ³•éœ€è¦åœ¨é€Ÿåº¦å’Œè´¨é‡ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ç‚¹ã€‚

### 6.2 ä»£ç å®ç°

#### C++å®ç°
```cpp
Mat realtime_sr(const Mat& src, float scale_factor) {
    // Use fast bilinear interpolation
    int new_rows = round(src.rows * scale_factor);
    int new_cols = round(src.cols * scale_factor);
    Mat dst(new_rows, new_cols, src.type());

    // Process each channel
    vector<Mat> channels;
    split(src, channels);
    vector<Mat> upscaled_channels;

    #pragma omp parallel for
    for(int c = 0; c < channels.size(); c++) {
        Mat upscaled(new_rows, new_cols, CV_32F);

        // Fast bilinear interpolation
        for(int i = 0; i < new_rows; i++) {
            float y = i / scale_factor;
            int y0 = floor(y);
            int y1 = min(y0 + 1, src.rows - 1);
            float wy = y - y0;

            for(int j = 0; j < new_cols; j++) {
                float x = j / scale_factor;
                int x0 = floor(x);
                int x1 = min(x0 + 1, src.cols - 1);
                float wx = x - x0;

                // Bilinear interpolation
                float val = (1-wx)*(1-wy)*channels[c].at<uchar>(y0,x0) +
                           wx*(1-wy)*channels[c].at<uchar>(y0,x1) +
                           (1-wx)*wy*channels[c].at<uchar>(y1,x0) +
                           wx*wy*channels[c].at<uchar>(y1,x1);

                upscaled.at<float>(i,j) = val;
            }
        }

        upscaled.convertTo(upscaled, CV_8U);
        upscaled_channels.push_back(upscaled);
    }

    merge(upscaled_channels, dst);
    return dst;
}
```

#### Pythonå®ç°
```python
def realtime_super_resolution(src: np.ndarray, scale: float = 2.0) -> np.ndarray:
    """å®æ—¶è¶…åˆ†è¾¨ç‡

    Args:
        src: è¾“å…¥å›¾åƒ
        scale: æ”¾å¤§å€æ•°

    Returns:
        np.ndarray: è¶…åˆ†è¾¨ç‡åçš„å›¾åƒ
    """
    # ä½¿ç”¨å¿«é€Ÿçš„åŒçº¿æ€§æ’å€¼
    h, w = src.shape[:2]
    new_h, new_w = int(h * scale), int(w * scale)

    # åˆ›å»ºè¾“å‡ºå›¾åƒ
    dst = np.zeros((new_h, new_w, 3), dtype=np.uint8)

    # å¿«é€ŸåŒçº¿æ€§æ’å€¼
    for i in range(new_h):
        for j in range(new_w):
            # è®¡ç®—å¯¹åº”çš„è¾“å…¥å›¾åƒåæ ‡
            x = j / scale
            y = i / scale

            # è·å–å››ä¸ªç›¸é‚»åƒç´ 
            x0, y0 = int(x), int(y)
            x1 = min(x0 + 1, w - 1)
            y1 = min(y0 + 1, h - 1)

            # è®¡ç®—æƒé‡
            wx = x - x0
            wy = y - y0

            # åŒçº¿æ€§æ’å€¼
            dst[i,j] = (1-wx)*(1-wy)*src[y0,x0] + \
                      wx*(1-wy)*src[y0,x1] + \
                      (1-wx)*wy*src[y1,x0] + \
                      wx*wy*src[y1,x1]

    return dst
```

## æ€»ç»“

å›¾åƒè¶…åˆ†è¾¨ç‡å°±åƒæ˜¯æ•°å­—ä¸–ç•Œçš„"æ™ºèƒ½æ”¾å¤§é•œ"ï¼é€šè¿‡ä¼ ç»Ÿæ–¹æ³•ã€æ·±åº¦å­¦ä¹ å’Œè§†é¢‘å¤„ç†ç­‰"æ”¾å¤§æŠ€æœ¯"ï¼Œæˆ‘ä»¬å¯ä»¥è®©ä½åˆ†è¾¨ç‡å›¾åƒé‡ç°æ¸…æ™°ç»†èŠ‚ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œéœ€è¦æ ¹æ®å…·ä½“åœºæ™¯é€‰æ‹©åˆé€‚çš„"æ”¾å¤§æ–¹æ¡ˆ"ï¼Œå°±åƒé€‰æ‹©åˆé€‚å€æ•°çš„æ”¾å¤§é•œä¸€æ ·ã€‚

è®°ä½ï¼šå¥½çš„è¶…åˆ†è¾¨ç‡æŠ€æœ¯å°±åƒæ˜¯ä¸€ä¸ªæ™ºèƒ½çš„"æ”¾å¤§é•œ"ï¼Œæ—¢è¦æå‡åˆ†è¾¨ç‡ï¼Œåˆè¦ä¿æŒå›¾åƒçš„çœŸå®æ€§ï¼ğŸ”

## å‚è€ƒèµ„æ–™

1. Dong C, et al. Learning a deep convolutional network for image super-resolution[C]. ECCV, 2014
2. Kim J, et al. Accurate image super-resolution using very deep convolutional networks[C]. CVPR, 2016
3. Lim B, et al. Enhanced deep residual networks for single image super-resolution[C]. CVPRW, 2017
4. Wang X, et al. ESRGAN: Enhanced super-resolution generative adversarial networks[C]. ECCVW, 2018
5. OpenCVå®˜æ–¹æ–‡æ¡£: https://docs.opencv.org/
6. æ›´å¤šèµ„æº: [IP101é¡¹ç›®ä¸»é¡µ](https://github.com/GlimmerLab/IP101)