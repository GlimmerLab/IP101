# ç›®æ ‡æ£€æµ‹ä»£ç å®ç°æŒ‡å— ğŸ¯

æœ¬æ–‡æ¡£æä¾›äº†ç›®æ ‡æ£€æµ‹ç®—æ³•çš„Pythonå’ŒC++å®Œæ•´å®ç°ä»£ç ã€‚æ¯ä¸ªå®ç°éƒ½åŒ…å«äº†è¯¦ç»†çš„æ³¨é‡Šè¯´æ˜å’Œå‚æ•°è§£é‡Šã€‚

## ç›®å½•
- [1. Pythonå®ç°](#1-pythonå®ç°)
  - [1.1 æ»‘åŠ¨çª—å£æ£€æµ‹](#11-æ»‘åŠ¨çª—å£æ£€æµ‹)
  - [1.2 HOG+SVMæ£€æµ‹](#12-hogsvmæ£€æµ‹)
  - [1.3 Haar+AdaBoostæ£€æµ‹](#13-haaradaboostæ£€æµ‹)
  - [1.4 éæå¤§å€¼æŠ‘åˆ¶](#14-éæå¤§å€¼æŠ‘åˆ¶)
  - [1.5 ç›®æ ‡è·Ÿè¸ª](#15-ç›®æ ‡è·Ÿè¸ª)
- [2. C++å®ç°](#2-cå®ç°)
  - [2.1 æ»‘åŠ¨çª—å£æ£€æµ‹](#21-æ»‘åŠ¨çª—å£æ£€æµ‹)
  - [2.2 HOG+SVMæ£€æµ‹](#22-hogsvmæ£€æµ‹)
  - [2.3 Haar+AdaBoostæ£€æµ‹](#23-haaradaboostæ£€æµ‹)
  - [2.4 éæå¤§å€¼æŠ‘åˆ¶](#24-éæå¤§å€¼æŠ‘åˆ¶)
  - [2.5 ç›®æ ‡è·Ÿè¸ª](#25-ç›®æ ‡è·Ÿè¸ª)

## 1. Pythonå®ç°

### 1.1 æ»‘åŠ¨çª—å£æ£€æµ‹

```python
import cv2
import numpy as np

def sliding_window_detection(img_path: str, window_size: tuple = (64, 64), stride: int = 32) -> np.ndarray:
    """
    ä½¿ç”¨æ»‘åŠ¨çª—å£è¿›è¡Œç›®æ ‡æ£€æµ‹

    å‚æ•°:
        img_path: str, è¾“å…¥å›¾åƒè·¯å¾„
        window_size: tuple, çª—å£å¤§å°ï¼Œé»˜è®¤(64, 64)
        stride: int, æ­¥é•¿ï¼Œé»˜è®¤32

    è¿”å›:
        np.ndarray: æ£€æµ‹ç»“æœå¯è§†åŒ–å›¾åƒ
    """
    # è¯»å–å›¾åƒ
    img = cv2.imread(img_path)
    if img is None:
        raise ValueError(f"æ— æ³•è¯»å–å›¾åƒ: {img_path}")

    # å¤åˆ¶åŸå›¾ç”¨äºç»˜åˆ¶ç»“æœ
    result = img.copy()

    # è½¬æ¢ä¸ºç°åº¦å›¾
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    # å®šä¹‰ç®€å•çš„ç›®æ ‡æ£€æµ‹å‡½æ•°ï¼ˆè¿™é‡Œä½¿ç”¨è¾¹ç¼˜æ£€æµ‹ä½œä¸ºç¤ºä¾‹ï¼‰
    def detect_object(window):
        edges = cv2.Canny(window, 100, 200)
        return np.sum(edges) > 1000  # ç®€å•çš„é˜ˆå€¼åˆ¤æ–­

    # æ»‘åŠ¨çª—å£æ£€æµ‹
    for y in range(0, img.shape[0] - window_size[1], stride):
        for x in range(0, img.shape[1] - window_size[0], stride):
            # æå–çª—å£
            window = gray[y:y+window_size[1], x:x+window_size[0]]

            # æ£€æµ‹ç›®æ ‡
            if detect_object(window):
                # ç»˜åˆ¶æ£€æµ‹æ¡†
                cv2.rectangle(result, (x, y),
                            (x + window_size[0], y + window_size[1]),
                            (0, 255, 0), 2)

    return result
```

### 1.2 HOG+SVMæ£€æµ‹

```python
from sklearn import svm
from sklearn.preprocessing import StandardScaler

def hog_svm_detection(img_path: str, window_size: tuple = (64, 64), stride: int = 32) -> np.ndarray:
    """
    ä½¿ç”¨HOGç‰¹å¾å’ŒSVMè¿›è¡Œç›®æ ‡æ£€æµ‹

    å‚æ•°:
        img_path: str, è¾“å…¥å›¾åƒè·¯å¾„
        window_size: tuple, çª—å£å¤§å°ï¼Œé»˜è®¤(64, 64)
        stride: int, æ­¥é•¿ï¼Œé»˜è®¤32

    è¿”å›:
        np.ndarray: æ£€æµ‹ç»“æœå¯è§†åŒ–å›¾åƒ
    """
    # è¯»å–å›¾åƒ
    img = cv2.imread(img_path)
    if img is None:
        raise ValueError(f"æ— æ³•è¯»å–å›¾åƒ: {img_path}")

    # å¤åˆ¶åŸå›¾ç”¨äºç»˜åˆ¶ç»“æœ
    result = img.copy()

    # è½¬æ¢ä¸ºç°åº¦å›¾
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    # åˆ›å»ºHOGæè¿°å­
    hog = cv2.HOGDescriptor()

    # åˆ›å»ºå’Œè®­ç»ƒSVMï¼ˆè¿™é‡Œä½¿ç”¨ç®€å•çš„ç¤ºä¾‹æ•°æ®ï¼‰
    # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œéœ€è¦ä½¿ç”¨å¤§é‡çš„æ­£è´Ÿæ ·æœ¬è¿›è¡Œè®­ç»ƒ
    svm_classifier = svm.LinearSVC()
    scaler = StandardScaler()

    # ç”Ÿæˆä¸€äº›ç¤ºä¾‹æ•°æ®è¿›è¡Œè®­ç»ƒ
    n_samples = 100
    n_features = hog.compute(cv2.resize(gray, window_size)).shape[0]
    X = np.random.randn(n_samples, n_features)
    y = np.random.randint(0, 2, n_samples)

    # è®­ç»ƒSVM
    X_scaled = scaler.fit_transform(X)
    svm_classifier.fit(X_scaled, y)

    # æ»‘åŠ¨çª—å£æ£€æµ‹
    for y in range(0, img.shape[0] - window_size[1], stride):
        for x in range(0, img.shape[1] - window_size[0], stride):
            # æå–çª—å£
            window = gray[y:y+window_size[1], x:x+window_size[0]]
            window = cv2.resize(window, window_size)

            # è®¡ç®—HOGç‰¹å¾
            features = hog.compute(window)

            # ç‰¹å¾æ ‡å‡†åŒ–
            features_scaled = scaler.transform(features.reshape(1, -1))

            # SVMé¢„æµ‹
            if svm_classifier.predict(features_scaled)[0] == 1:
                # ç»˜åˆ¶æ£€æµ‹æ¡†
                cv2.rectangle(result, (x, y),
                            (x + window_size[0], y + window_size[1]),
                            (0, 255, 0), 2)

    return result
```

### 1.3 Haar+AdaBoostæ£€æµ‹

```python
def haar_adaboost_detection(img_path: str) -> np.ndarray:
    """
    ä½¿ç”¨Haarç‰¹å¾å’ŒAdaBoostè¿›è¡Œç›®æ ‡æ£€æµ‹

    å‚æ•°:
        img_path: str, è¾“å…¥å›¾åƒè·¯å¾„

    è¿”å›:
        np.ndarray: æ£€æµ‹ç»“æœå¯è§†åŒ–å›¾åƒ
    """
    # è¯»å–å›¾åƒ
    img = cv2.imread(img_path)
    if img is None:
        raise ValueError(f"æ— æ³•è¯»å–å›¾åƒ: {img_path}")

    # å¤åˆ¶åŸå›¾ç”¨äºç»˜åˆ¶ç»“æœ
    result = img.copy()

    # è½¬æ¢ä¸ºç°åº¦å›¾
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    # åŠ è½½é¢„è®­ç»ƒçš„äººè„¸æ£€æµ‹å™¨ï¼ˆä½œä¸ºç¤ºä¾‹ï¼‰
    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

    # è¿›è¡Œäººè„¸æ£€æµ‹
    faces = face_cascade.detectMultiScale(gray, 1.1, 4)

    # ç»˜åˆ¶æ£€æµ‹ç»“æœ
    for (x, y, w, h) in faces:
        cv2.rectangle(result, (x, y), (x+w, y+h), (0, 255, 0), 2)

    return result
```

### 1.4 éæå¤§å€¼æŠ‘åˆ¶

```python
def compute_nms_manual(boxes: np.ndarray, scores: np.ndarray, iou_threshold: float = 0.5) -> list:
    """
    æ‰‹åŠ¨å®ç°éæå¤§å€¼æŠ‘åˆ¶ç®—æ³•

    å‚æ•°:
        boxes: np.ndarray, è¾¹ç•Œæ¡†åæ ‡åˆ—è¡¨ï¼Œæ¯ä¸ªæ¡†ä¸º[x1, y1, x2, y2]æ ¼å¼
        scores: np.ndarray, æ¯ä¸ªè¾¹ç•Œæ¡†å¯¹åº”çš„ç½®ä¿¡åº¦åˆ†æ•°
        iou_threshold: float, IoUé˜ˆå€¼ï¼Œé»˜è®¤0.5

    è¿”å›:
        list: ä¿ç•™çš„è¾¹ç•Œæ¡†ç´¢å¼•åˆ—è¡¨
    """
    # ç¡®ä¿è¾“å…¥æ•°æ®æ ¼å¼æ­£ç¡®
    boxes = np.array(boxes)
    scores = np.array(scores)

    # è®¡ç®—æ‰€æœ‰æ¡†çš„é¢ç§¯
    x1 = boxes[:, 0]
    y1 = boxes[:, 1]
    x2 = boxes[:, 2]
    y2 = boxes[:, 3]
    areas = (x2 - x1) * (y2 - y1)

    # æ ¹æ®åˆ†æ•°æ’åº
    order = scores.argsort()[::-1]

    keep = []
    while order.size > 0:
        # ä¿ç•™åˆ†æ•°æœ€é«˜çš„æ¡†
        i = order[0]
        keep.append(i)

        if order.size == 1:
            break

        # è®¡ç®—å…¶ä»–æ¡†ä¸å½“å‰æ¡†çš„IoU
        xx1 = np.maximum(x1[i], x1[order[1:]])
        yy1 = np.maximum(y1[i], y1[order[1:]])
        xx2 = np.minimum(x2[i], x2[order[1:]])
        yy2 = np.minimum(y2[i], y2[order[1:]])

        w = np.maximum(0.0, xx2 - xx1)
        h = np.maximum(0.0, yy2 - yy1)
        inter = w * h

        # è®¡ç®—IoU
        ovr = inter / (areas[i] + areas[order[1:]] - inter)

        # ä¿ç•™IoUå°äºé˜ˆå€¼çš„æ¡†
        inds = np.where(ovr <= iou_threshold)[0]
        order = order[inds + 1]

    return keep
```

### 1.5 ç›®æ ‡è·Ÿè¸ª

```python
def object_tracking(img_path: str, video_path: str = None) -> np.ndarray:
    """
    å®ç°ç®€å•çš„ç›®æ ‡è·Ÿè¸ªç®—æ³•

    å‚æ•°:
        img_path: str, ç›®æ ‡æ¨¡æ¿å›¾åƒè·¯å¾„
        video_path: str, è§†é¢‘è·¯å¾„ï¼Œé»˜è®¤ä¸ºNoneï¼ˆä½¿ç”¨æ‘„åƒå¤´ï¼‰

    è¿”å›:
        np.ndarray: è·Ÿè¸ªç»“æœå¯è§†åŒ–å›¾åƒ
    """
    # è¯»å–ç›®æ ‡æ¨¡æ¿
    template = cv2.imread(img_path)
    if template is None:
        raise ValueError(f"æ— æ³•è¯»å–æ¨¡æ¿å›¾åƒ: {img_path}")

    # è½¬æ¢ä¸ºç°åº¦å›¾
    template_gray = cv2.cvtColor(template, cv2.COLOR_BGR2GRAY)

    # åˆå§‹åŒ–è§†é¢‘æ•è·
    if video_path is None:
        cap = cv2.VideoCapture(0)  # ä½¿ç”¨æ‘„åƒå¤´
    else:
        cap = cv2.VideoCapture(video_path)

    # è¯»å–ç¬¬ä¸€å¸§
    ret, frame = cap.read()
    if not ret:
        raise ValueError("æ— æ³•è¯»å–è§†é¢‘å¸§")

    # é€‰æ‹©åˆå§‹è·Ÿè¸ªåŒºåŸŸï¼ˆè¿™é‡Œä½¿ç”¨æ¨¡æ¿å¤§å°ï¼‰
    h, w = template_gray.shape
    x, y = frame.shape[1]//2 - w//2, frame.shape[0]//2 - h//2
    track_window = (x, y, w, h)

    # è®¾ç½®ç»ˆæ­¢æ¡ä»¶
    term_crit = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1)

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        # ä½¿ç”¨æ¨¡æ¿åŒ¹é…è¿›è¡Œè·Ÿè¸ª
        result = cv2.matchTemplate(frame_gray, template_gray, cv2.TM_CCOEFF_NORMED)
        min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)

        # æ›´æ–°è·Ÿè¸ªçª—å£
        track_window = (max_loc[0], max_loc[1], w, h)

        # ç»˜åˆ¶è·Ÿè¸ªæ¡†
        x, y, w, h = track_window
        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)

        # æ˜¾ç¤ºç»“æœ
        cv2.imshow('Tracking', frame)

        # æŒ‰'q'é€€å‡º
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()

    return frame
```

## 2. C++å®ç°

### 2.1 æ»‘åŠ¨çª—å£æ£€æµ‹

```cpp
#include <opencv2/opencv.hpp>
#include <vector>

cv::Mat slidingWindowDetection(const std::string& imgPath,
                             const cv::Size& windowSize = cv::Size(64, 64),
                             int stride = 32) {
    // è¯»å–å›¾åƒ
    cv::Mat img = cv::imread(imgPath);
    if (img.empty()) {
        throw std::runtime_error("æ— æ³•è¯»å–å›¾åƒ");
    }

    // å¤åˆ¶åŸå›¾ç”¨äºç»˜åˆ¶ç»“æœ
    cv::Mat result = img.clone();

    // è½¬æ¢ä¸ºç°åº¦å›¾
    cv::Mat gray;
    cv::cvtColor(img, gray, cv::COLOR_BGR2GRAY);

    // å®šä¹‰ç®€å•çš„ç›®æ ‡æ£€æµ‹å‡½æ•°
    auto detectObject = [](const cv::Mat& window) {
        cv::Mat edges;
        cv::Canny(window, edges, 100, 200);
        return cv::sum(edges)[0] > 1000;  // ç®€å•çš„é˜ˆå€¼åˆ¤æ–­
    };

    // æ»‘åŠ¨çª—å£æ£€æµ‹
    for (int y = 0; y < img.rows - windowSize.height; y += stride) {
        for (int x = 0; x < img.cols - windowSize.width; x += stride) {
            // æå–çª—å£
            cv::Mat window = gray(cv::Rect(x, y, windowSize.width, windowSize.height));

            // æ£€æµ‹ç›®æ ‡
            if (detectObject(window)) {
                // ç»˜åˆ¶æ£€æµ‹æ¡†
                cv::rectangle(result,
                            cv::Point(x, y),
                            cv::Point(x + windowSize.width, y + windowSize.height),
                            cv::Scalar(0, 255, 0), 2);
            }
        }
    }

    return result;
}
```

### 2.2 HOG+SVMæ£€æµ‹

```cpp
cv::Mat hogSvmDetection(const std::string& imgPath,
                       const cv::Size& windowSize = cv::Size(64, 64),
                       int stride = 32) {
    // è¯»å–å›¾åƒ
    cv::Mat img = cv::imread(imgPath);
    if (img.empty()) {
        throw std::runtime_error("æ— æ³•è¯»å–å›¾åƒ");
    }

    // å¤åˆ¶åŸå›¾ç”¨äºç»˜åˆ¶ç»“æœ
    cv::Mat result = img.clone();

    // è½¬æ¢ä¸ºç°åº¦å›¾
    cv::Mat gray;
    cv::cvtColor(img, gray, cv::COLOR_BGR2GRAY);

    // åˆ›å»ºHOGæè¿°å­
    cv::HOGDescriptor hog;
    hog.setSVMDetector(cv::HOGDescriptor::getDefaultPeopleDetector());

    // æ£€æµ‹ç›®æ ‡
    std::vector<cv::Rect> detections;
    std::vector<double> weights;
    hog.detectMultiScale(img, detections, weights);

    // ç»˜åˆ¶æ£€æµ‹ç»“æœ
    for (const auto& rect : detections) {
        cv::rectangle(result, rect, cv::Scalar(0, 255, 0), 2);
    }

    return result;
}
```

### 2.3 Haar+AdaBoostæ£€æµ‹

```cpp
cv::Mat haarAdaboostDetection(const std::string& imgPath) {
    // è¯»å–å›¾åƒ
    cv::Mat img = cv::imread(imgPath);
    if (img.empty()) {
        throw std::runtime_error("æ— æ³•è¯»å–å›¾åƒ");
    }

    // å¤åˆ¶åŸå›¾ç”¨äºç»˜åˆ¶ç»“æœ
    cv::Mat result = img.clone();

    // è½¬æ¢ä¸ºç°åº¦å›¾
    cv::Mat gray;
    cv::cvtColor(img, gray, cv::COLOR_BGR2GRAY);

    // åŠ è½½é¢„è®­ç»ƒçš„äººè„¸æ£€æµ‹å™¨
    cv::CascadeClassifier faceCascade;
    faceCascade.load(cv::samples::findFile("haarcascade_frontalface_default.xml"));

    // æ£€æµ‹äººè„¸
    std::vector<cv::Rect> faces;
    faceCascade.detectMultiScale(gray, faces, 1.1, 4);

    // ç»˜åˆ¶æ£€æµ‹ç»“æœ
    for (const auto& rect : faces) {
        cv::rectangle(result, rect, cv::Scalar(0, 255, 0), 2);
    }

    return result;
}
```

### 2.4 éæå¤§å€¼æŠ‘åˆ¶

```cpp
std::vector<int> computeNmsManual(const std::vector<cv::Rect>& boxes,
                                 const std::vector<float>& scores,
                                 float iouThreshold = 0.5) {
    std::vector<int> keep;
    if (boxes.empty()) return keep;

    // è®¡ç®—æ‰€æœ‰æ¡†çš„é¢ç§¯
    std::vector<float> areas;
    for (const auto& box : boxes) {
        areas.push_back(box.width * box.height);
    }

    // åˆ›å»ºç´¢å¼•æ•°ç»„å¹¶æ ¹æ®åˆ†æ•°æ’åº
    std::vector<size_t> order(scores.size());
    std::iota(order.begin(), order.end(), 0);
    std::sort(order.begin(), order.end(),
              [&scores](size_t i1, size_t i2) { return scores[i1] > scores[i2]; });

    while (!order.empty()) {
        int i = order[0];
        keep.push_back(i);

        if (order.size() == 1) break;

        std::vector<size_t> new_order;
        for (size_t j = 1; j < order.size(); j++) {
            int n = order[j];

            // è®¡ç®—IoU
            int xx1 = std::max(boxes[i].x, boxes[n].x);
            int yy1 = std::max(boxes[i].y, boxes[n].y);
            int xx2 = std::min(boxes[i].x + boxes[i].width,
                             boxes[n].x + boxes[n].width);
            int yy2 = std::min(boxes[i].y + boxes[i].height,
                             boxes[n].y + boxes[n].height);

            int w = std::max(0, xx2 - xx1);
            int h = std::max(0, yy2 - yy1);
            float inter = w * h;
            float ovr = inter / (areas[i] + areas[n] - inter);

            if (ovr <= iouThreshold) {
                new_order.push_back(n);
            }
        }

        order = new_order;
    }

    return keep;
}
```

### 2.5 ç›®æ ‡è·Ÿè¸ª

```cpp
cv::Mat objectTracking(const std::string& imgPath, const std::string& videoPath = "") {
    // è¯»å–ç›®æ ‡æ¨¡æ¿
    cv::Mat templ = cv::imread(imgPath);
    if (templ.empty()) {
        throw std::runtime_error("æ— æ³•è¯»å–æ¨¡æ¿å›¾åƒ");
    }

    // è½¬æ¢ä¸ºç°åº¦å›¾
    cv::Mat templGray;
    cv::cvtColor(templ, templGray, cv::COLOR_BGR2GRAY);

    // åˆå§‹åŒ–è§†é¢‘æ•è·
    cv::VideoCapture cap;
    if (videoPath.empty()) {
        cap.open(0);  // ä½¿ç”¨æ‘„åƒå¤´
    } else {
        cap.open(videoPath);
    }

    if (!cap.isOpened()) {
        throw std::runtime_error("æ— æ³•æ‰“å¼€è§†é¢‘æº");
    }

    cv::Mat frame, result;
    cap >> frame;
    if (frame.empty()) {
        throw std::runtime_error("æ— æ³•è¯»å–è§†é¢‘å¸§");
    }

    // é€‰æ‹©åˆå§‹è·Ÿè¸ªåŒºåŸŸ
    int w = templGray.cols;
    int h = templGray.rows;
    int x = frame.cols/2 - w/2;
    int y = frame.rows/2 - h/2;
    cv::Rect trackWindow(x, y, w, h);

    // è®¾ç½®ç»ˆæ­¢æ¡ä»¶
    cv::TermCriteria termCrit(cv::TermCriteria::COUNT | cv::TermCriteria::EPS, 10, 1);

    while (true) {
        cap >> frame;
        if (frame.empty()) break;

        result = frame.clone();

        // è½¬æ¢ä¸ºç°åº¦å›¾
        cv::Mat frameGray;
        cv::cvtColor(frame, frameGray, cv::COLOR_BGR2GRAY);

        // ä½¿ç”¨æ¨¡æ¿åŒ¹é…è¿›è¡Œè·Ÿè¸ª
        cv::Mat matchResult;
        cv::matchTemplate(frameGray, templGray, matchResult, cv::TM_CCOEFF_NORMED);

        double minVal, maxVal;
        cv::Point minLoc, maxLoc;
        cv::minMaxLoc(matchResult, &minVal, &maxVal, &minLoc, &maxLoc);

        // æ›´æ–°è·Ÿè¸ªçª—å£
        trackWindow = cv::Rect(maxLoc.x, maxLoc.y, w, h);

        // ç»˜åˆ¶è·Ÿè¸ªæ¡†
        cv::rectangle(result, trackWindow, cv::Scalar(0, 255, 0), 2);

        // æ˜¾ç¤ºç»“æœ
        cv::imshow("Tracking", result);

        // æŒ‰'q'é€€å‡º
        if (cv::waitKey(1) == 'q') break;
    }

    cap.release();
    cv::destroyAllWindows();

    return result;
}
```